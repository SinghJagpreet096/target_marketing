### Introduction

The code and analysis are in the [Final_notebook.ipynb](https://github.com/SinghJagpreet096/target_marketing/blob/main/Final_notebook.ipynb).

    
Bank "A" is planning to run a marketing campaign aimed at its current customers with the objective of boosting their deposit balances. The campaign will offer special interest rates as a promotion to entice customers to increase their credit balances.The dataset contains real customer data from an international bank and the task is to find right set of customer who are most likely to subscribe for the campaign.To predict this we are using different classification models such as Logistic regression, Decision Tree clasifier, Random forest classifier , AdaBoost Classifier.
    
    
***Logistic Regression*** is a statistical method that models the relationship between a binary response variable (0 or 1) and one or more predictor variables. It estimates the probability of the response variable is 1, given the values of the predictor variables. The model uses a logistic function to transform a linear combination of the predictor variables into a probability value between 0 and 1. The logistic function is also known as the sigmoid function. The parameters of the model are estimated using maximum likelihood estimation.

***Decision Tree Classifier*** is a machine learning algorithm that creates a tree-like model of decisions and their possible consequences. It works by recursively splitting the data based on the feature that maximizes the separation between the classes. The nodes represent the decision rules based on the values of the features, and the leaves represent the class labels. The model can handle categorical and numerical data and is easy to interpret and visualize. It can also handle missing data by imputing the missing values.

***Random Forest Classifier*** is an ensemble learning algorithm that creates multiple decision trees and aggregates their predictions to make a final prediction. Each tree is built on a random subset of features and data, which reduces the overfitting problem. The algorithm uses bagging (bootstrap aggregation) to create the random subsets of data. The final prediction is based on the majority vote of the individual tree predictions. Random forest can handle both categorical and numerical data, is robust to outliers and missing values, and has high accuracy and low variance.

***Adaboost (Adaptive Boosting) Classifier*** is an ensemble learning algorithm that combines weak classifiers to create a strong classifier. It works by iteratively training weak classifiers on the same dataset and assigning higher weights to the misclassified samples in each iteration. The algorithm then combines the weak classifiers based on their accuracy and assigns a weight to each weak classifier proportional to its performance. The final prediction is based on the weighted sum of the individual weak classifier predictions. Adaboost can handle both categorical and numerical data and is less prone to overfitting than individual weak classifiers. It is also computationally efficient and has high accuracy.

***XGBoost (Extreme Gradient Boosting) Classifier*** is a powerful machine learning algorithm that uses a gradient boosting framework to create an ensemble of decision trees. It works by iteratively training decision trees on the residuals of the previous tree and assigning higher weights to the misclassified samples. The algorithm also includes regularization terms to control overfitting and uses a novel split finding algorithm to optimize the tree structure. XGBoost is computationally efficient, scalable, and can handle both categorical and numerical data.
